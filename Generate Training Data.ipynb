{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeff/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jeff/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jeff/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jeff/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jeff/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jeff/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jeff/.local/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "from config import *\n",
    "import random\n",
    "from itertools import combinations,permutations\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jeff/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape=[batch_size, N])\n",
    "y = tf.placeholder(tf.float32, shape=[batch_size, K])\n",
    "R_init = tf.placeholder(tf.float32, shape=[batch_size, N])\n",
    "\n",
    "net_dict = {}\n",
    "bp_iter_num = 5\n",
    "RNN = 1\n",
    "ss = np.hstack([0,np.arange(n)[::-1]+1])\n",
    "int_L = 5\n",
    "deci_L = 2\n",
    "step = 2**-deci_L\n",
    "_min = -2**(int_L+deci_L)*step\n",
    "_max = (2**(int_L+deci_L)-1)*step\n",
    "fix = 0\n",
    "if(fix):\n",
    "    inf_num = 2**(int_L-1)\n",
    "else:\n",
    "    inf_num = 1000\n",
    "\n",
    "# initial\n",
    "for i in range(n+1):\n",
    "    for j in range(N):\n",
    "        net_dict[\"L_{0}{1}{2}\".format(i,j,0)] = tf.zeros((batch_size))\n",
    "        net_dict[\"R_{0}{1}{2}\".format(i,j,0)] = tf.zeros((batch_size))\n",
    "\n",
    "if(RNN):\n",
    "    LV = tf.Variable(np.float32(np.ones((n,N,1))))\n",
    "    RV = tf.Variable(np.float32(np.ones((n,N,1))))\n",
    "else:\n",
    "    LV = tf.Variable(np.float32(np.ones((n,N,bp_iter_num))))\n",
    "    RV = tf.Variable(np.float32(np.ones((n,N,bp_iter_num))))\n",
    "\n",
    "for j in range(N):\n",
    "    net_dict[\"L_{0}{1}{2}\".format(n,j,0)] = tf.ones((1))*x[:,j]    \n",
    "    net_dict[\"R_{0}{1}{2}\".format(0,j,0)] = R_init[:,j]*inf_num\n",
    "\n",
    "for i in range(n+1):\n",
    "    for j in range(N):\n",
    "        for k in range(bp_iter_num):\n",
    "            net_dict[\"output_L_{0}{1}{2}\".format(i,j,k)] = tf.zeros((batch_size))\n",
    "            net_dict[\"output_R_{0}{1}{2}\".format(i,j,k)] = tf.zeros((batch_size))\n",
    "            \n",
    "# bp algorithm\n",
    "for k in range(bp_iter_num):\n",
    "    if(RNN):\n",
    "        itr = 0\n",
    "    else:\n",
    "        itr = k\n",
    "    for i in range(n,0,-1):\n",
    "        for phi in range(2**ss[i]):\n",
    "            psi = int(np.floor(phi/2))\n",
    "            if(np.mod(phi,2)!=0):\n",
    "                for omega in range(2**(n-ss[i])):\n",
    "                    net_dict[\"R_{0}{1}{2}\".format(n+1-i,psi+2*omega*2**(ss[i]-1),0)] = RV[n-i,psi+2*omega*2**(ss[i]-1),itr]*fFunction(net_dict[\"L_{0}{1}{2}\".format(n+1-i,psi+(2*omega+1)*2**(ss[i]-1),0)]+net_dict[\"R_{0}{1}{2}\".format(n-i,psi+(2*omega+1)*2**(ss[i]-1),0)], net_dict[\"R_{0}{1}{2}\".format(n-i,psi+2*omega*2**(ss[i]-1),0)])\n",
    "                    net_dict[\"R_{0}{1}{2}\".format(n+1-i,psi+(2*omega+1)*2**(ss[i]-1),0)] = net_dict[\"R_{0}{1}{2}\".format(n-i,psi+(2*omega+1)*2**(ss[i]-1),0)]+RV[n-i,psi+(2*omega+1)*2**(ss[i]-1),itr]*fFunction(net_dict[\"L_{0}{1}{2}\".format(n+1-i,psi+2*omega*2**(ss[i]-1),0)],net_dict[\"R_{0}{1}{2}\".format(n-i,psi+2*omega*2**(ss[i]-1),0)])\n",
    "                    \n",
    "    for i in range(1,n+1):\n",
    "        for phi in range(2**ss[i]):\n",
    "            psi = int(np.floor(phi/2))\n",
    "            if(np.mod(phi,2)!=0):\n",
    "                for omega in range(2**(n-ss[i])):\n",
    "                    net_dict[\"L_{0}{1}{2}\".format(n-i,psi+2*omega*2**(ss[i]-1),0)] = LV[n-i,psi+2*omega*2**(ss[i]-1),itr]*fFunction(net_dict[\"L_{0}{1}{2}\".format(n+1-i,psi+2*omega*2**(ss[i]-1),0)],net_dict[\"L_{0}{1}{2}\".format(n+1-i,psi+(2*omega+1)*2**(ss[i]-1),0)]+net_dict[\"R_{0}{1}{2}\".format(n-i,psi+(2*omega+1)*2**(ss[i]-1),0)])             \n",
    "                    net_dict[\"L_{0}{1}{2}\".format(n-i,psi+(2*omega+1)*2**(ss[i]-1),0)] = net_dict[\"L_{0}{1}{2}\".format(n+1-i,psi+(2*omega+1)*2**(ss[i]-1),0)]+LV[n-i,psi+(2*omega+1)*2**(ss[i]-1),itr]*fFunction(net_dict[\"L_{0}{1}{2}\".format(n+1-i,psi+2*omega*2**(ss[i]-1),0)],net_dict[\"R_{0}{1}{2}\".format(n-i,psi+2*omega*2**(ss[i]-1),0)])\n",
    "\n",
    "    for i in range(n+1):\n",
    "        for j in range(N):\n",
    "            net_dict[\"output_L_{0}{1}{2}\".format(i,j,k)] = net_dict[\"L_{0}{1}{2}\".format(i,j,0)]\n",
    "            net_dict[\"output_R_{0}{1}{2}\".format(i,j,k)] = net_dict[\"R_{0}{1}{2}\".format(i,j,0)]\n",
    "            \n",
    "y_output = tf.zeros((1))\n",
    "for i in range(N):\n",
    "    if(FZlookup[i] == -1):\n",
    "        y_output = tf.concat([y_output,net_dict[\"L_{0}{1}{2}\".format(0,i,0)]],0)\n",
    "y_output = tf.transpose(tf.reshape(y_output[1:],(K,batch_size)))*-1\n",
    "loss = 1.0*tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_output,labels=y))\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))  #allow tensorflow to automatically allocate device\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2arr(_net_dict):\n",
    "    output = np.zeros((batch_size, n+1, N, bp_iter_num, 2))\n",
    "    for i in range(n+1):\n",
    "        for j in range(N):\n",
    "            for k in range(bp_iter_num):\n",
    "                output[:,i,j,k,0] = _net_dict[\"output_L_{0}{1}{2}\".format(i,j,k)]\n",
    "                output[:,i,j,k,1] = _net_dict[\"output_R_{0}{1}{2}\".format(i,j,k)]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_idx(err_map,indices):\n",
    "    a,b = np.where(err_map == -1)\n",
    "    i = 0\n",
    "    y = []\n",
    "    for c in np.unique(a):\n",
    "        idx = np.where(a == c)[0]\n",
    "        y.append(indices[b[idx]])\n",
    "    return np.unique(a),y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jeff/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from Model/16_0.0_5.0_5_1_0.ckpt\n",
      "0\n",
      "1\n",
      "At SNR: 0, Generated number of data: 3000\n",
      "0\n",
      "1\n",
      "2\n",
      "At SNR: 1, Generated number of data: 3000\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "At SNR: 2, Generated number of data: 3000\n",
      "Test SNR:        [0. 1. 2.]\n",
      "Test Loss:       [0.28986324 0.19284699 0.11169343]\n",
      "Test BER:        [0.1264875  0.07963722 0.04304542]\n",
      "Test FER:        [0.30168667 0.19290667 0.10596   ]\n",
      "Test BF_1 FER:   [0.27389333 0.17466222 0.09563333]\n",
      "22.870747566223145\n"
     ]
    }
   ],
   "source": [
    "tStart = time.time()\n",
    "max_flip = 1\n",
    "\n",
    "best_val_ber = 1\n",
    "load_weight = 1\n",
    "quantize_weight = 0 #0 for non-quantize, 1 for normal, 2 for binarized, 3 for bin, 4 for binarized bin\n",
    "bin_bit = 3  # number of different value\n",
    "binary_prec = 6 # binary precision, binary_prec must >= bin_bit\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "if(load_weight):\n",
    "    saver.restore(sess, 'Model/'+str(N)+'_0.0_5.0_5_1_0.ckpt')\n",
    "\n",
    "test_nfails = np.zeros((len(ebn0)))\n",
    "test_loss = np.zeros((len(ebn0)))\n",
    "test_nframe = np.zeros((len(ebn0)))\n",
    "BF_test_nfails = np.zeros(((len(ebn0)),max_flip))\n",
    "BF_test_nframe = np.zeros(((len(ebn0)),max_flip))\n",
    "y_pred_layer = np.zeros((batch_size,K,max_flip+1))\n",
    "fail_frame_layer = np.zeros((batch_size,max_flip+1))\n",
    "err_map = np.zeros((batch_size,K,max_flip+1))\n",
    "\n",
    "wordRandom = np.random.RandomState(word_seed-200)\n",
    "noiseRandom = np.random.RandomState(noise_seed-200)\n",
    "zero = np.zeros(batch_size)\n",
    "flip_bit_order = np.sort(indices[:K])\n",
    "\n",
    "data_num = 3000\n",
    "\n",
    "cnt_list = []\n",
    "for j in range(len(ebn0)):\n",
    "    X = []\n",
    "    Y = []\n",
    "    Z = []\n",
    "    cnt = 0\n",
    "    while(len(X)<data_num):\n",
    "        print(cnt)\n",
    "        cnt = cnt + 1\n",
    "        x_test, y_test = gendata(j,True,True)\n",
    "        r_layer = np.zeros((batch_size,N,max_flip+1))\n",
    "        l = 0 # layer 0\n",
    "        r_layer[:,FZlookup == 0,l] = 1\n",
    "        y_pred_layer[:,:,0], _loss, _net_dict = sess.run(fetches=[y_output, loss, net_dict], feed_dict={x: x_test, y: y_test, R_init: r_layer[:,:,l]})\n",
    "        test_loss[j] = _loss + test_loss[j]\n",
    "        uhat = np.zeros((batch_size,K))\n",
    "        uhat[y_pred_layer[:,:,0]>=0] = 1\n",
    "        fail_frame_layer[:,0] = np.logical_or.reduce(uhat!=y_test,1)\n",
    "        fail_frame_layer[:,1] = np.logical_or.reduce(uhat!=y_test,1)\n",
    "        test_nfails[j] = test_nfails[j] + sum(sum(uhat!=y_test))        \n",
    "        test_nframe[j] = test_nframe[j] + sum(fail_frame_layer[:,0])\n",
    "        output = dict2arr(_net_dict)\n",
    "        \n",
    "        for k in range(K):\n",
    "            l = 1 # layer 1\n",
    "            r_layer[:,:,l] = r_layer[:,:,l-1].copy()\n",
    "            flip_bit = flip_bit_order[k]\n",
    "            idx = len(np.where(FZlookup[:flip_bit] == -1)[0]) # get the right idx of y_pred\n",
    "            r_layer[:,flip_bit,l] = np.sign(y_pred_layer[:,idx,l-1])\n",
    "            y_pred_layer[:,:,l], _loss = sess.run(fetches=[y_output, loss], feed_dict={x: x_test, y: y_test, R_init: r_layer[:,:,l]})\n",
    "            uhat = np.zeros((batch_size,K))\n",
    "            uhat[y_pred_layer[:,:,l]>=0] = 1\n",
    "            fail_frame = np.logical_or.reduce(uhat!=y_test,1)\n",
    "            fail_frame_layer[:,1] = fail_frame_layer[:,1] * fail_frame\n",
    "            err_map[:,k,0] = fail_frame - fail_frame_layer[:,0] # find new corrected one\n",
    "                    \n",
    "        BF_test_nframe[j,0] = BF_test_nframe[j,0] + sum(fail_frame_layer[:,1])\n",
    "        a, b = get_data_idx(err_map[:,:,0],flip_bit_order)\n",
    "        X.extend(output[a,:,:,:])\n",
    "        Y.extend(b)\n",
    "        Z.extend(y_test[a,:])\n",
    "    X = np.array(X)[:data_num]\n",
    "    Y = Y[:data_num]\n",
    "    Z = Z[:data_num]\n",
    "    if(CRC):\n",
    "        pickle.dump(X, open(\"./Data/CRC_X_SNR_{}_{}_1_0.p\".format(N,int(ebn0[j])), \"wb\"))\n",
    "        pickle.dump(Y, open(\"./Data/CRC_Y_SNR_{}_{}_1_0.p\".format(N,int(ebn0[j])), \"wb\"))\n",
    "    else:\n",
    "        pickle.dump(X, open(\"./Data/X_SNR_{}_{}_1_0.p\".format(N,int(ebn0[j])), \"wb\"))\n",
    "        pickle.dump(Y, open(\"./Data/Y_SNR_{}_{}_1_0.p\".format(N,int(ebn0[j])), \"wb\"))\n",
    "        pickle.dump(Z, open(\"./Data/Z_SNR_{}_{}_1_0.p\".format(N,int(ebn0[j])), \"wb\"))\n",
    "    print('At SNR: {}, Generated number of data: {}'.format(int(ebn0[j]),len(Y)))\n",
    "    cnt_list.append(cnt)\n",
    "print('Test SNR:       ',ebn0)\n",
    "print('Test Loss:      ',test_loss/cnt_list)\n",
    "print('Test BER:       ',test_nfails/cnt_list/(batch_size*K))\n",
    "print('Test FER:       ',test_nframe/cnt_list/(batch_size))\n",
    "for i in range(max_flip):\n",
    "    print('Test BF_{0} FER:   {1}'.format(int(i+1),BF_test_nframe[:,i]/cnt_list/(batch_size)))\n",
    "tEnd = time.time()\n",
    "print(tEnd-tStart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test SNR:        [0. 1. 2.]\n",
      "Test Loss:       [0.28986324 0.19284699 0.11169343]\n",
      "Test BER:        [0.1264875  0.07963722 0.04304542]\n",
      "Test FER:        [0.30168667 0.19290667 0.10596   ]\n",
      "Test BF_1 FER:   [0.27389333 0.17466222 0.09563333]\n"
     ]
    }
   ],
   "source": [
    "print('Test SNR:       ',ebn0)\n",
    "print('Test Loss:      ',test_loss/cnt_list)\n",
    "print('Test BER:       ',test_nfails/cnt_list/(batch_size*K))\n",
    "print('Test FER:       ',test_nframe/cnt_list/(batch_size))\n",
    "for i in range(max_flip):\n",
    "    print('Test BF_{0} FER:   {1}'.format(int(i+1),BF_test_nframe[:,i]/cnt_list/(batch_size)))\n",
    "tEnd = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
